{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from auditus.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auditus\n",
    "\n",
    "> Simple Audio Embedding Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`auditus` gives you simple access to state-of-the-art audio embeddings. Like [SentenceTransformers](https://sbert.net/) for audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ pip install auditus\n",
    "```\n",
    "\n",
    "\n",
    "[repo]: https://github.com/CarloLepelaars/auditus\n",
    "[docs]: https://CarloLepelaars.github.io/auditus/\n",
    "[pypi]: https://pypi.org/project/auditus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level object in `auditus` is the `AudioPipeline` which takes in a path and returns a pooled embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8653, 1.1659, 0.5956, 0.8498, 0.5322])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditus.transform import AudioPipeline\n",
    "\n",
    "pipe = AudioPipeline(\n",
    "    # Default AST model\n",
    "    model_name=\"MIT/ast-finetuned-audioset-10-10-0.4593\", \n",
    "    # PyTorch output\n",
    "    return_tensors=\"pt\", \n",
    "    # Resampled to 16KhZ\n",
    "    target_sr=16000, \n",
    "    # Mean pooling to obtain single embedding vector\n",
    "    pooling=\"mean\",\n",
    ")\n",
    "\n",
    "output = pipe(\"../test_files/XC119042.ogg\").squeeze(0)\n",
    "print(output.shape)\n",
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see `AudioPipeline` in action on a practical use case, check out [this Kaggle Notebook for the BirdCLEF+ 2025 competition](https://www.kaggle.com/code/carlolepelaars/generating-audio-embeddings-with-auditus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`auditus` offers a range of transforms to process audio for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load audio with a given sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auditus.core.AudioArray(a=array([-2.64216160e-05, -2.54259703e-05,  5.56615578e-06, ...,\n",
       "       -2.03555092e-01, -2.03390077e-01, -2.45199591e-01]), sr=32000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditus.transform import AudioLoader\n",
    "\n",
    "audio = AudioLoader(sr=32000)(\"../test_files/XC119042.ogg\")\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AudioArray` object offers a convenient interface for audio data. For example, you can listen to the audio in Jupyter Notebooks with `audio.audio()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.64216160e-05, -2.54259703e-05,  5.56615578e-06, -5.17481631e-08,\n",
       "        -1.35020821e-06]),\n",
       " 32000,\n",
       " 632790)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.a[:5], audio.sr, len(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many Audio Transformer models work only on a specific sampling rate. With `Resampling` you can resample the audio to the desired sampling rate. Here we go from 32kHz to 16kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auditus.core.AudioArray(a=array([-2.64216160e-05,  5.56613802e-06, -1.35020873e-06, ...,\n",
       "       -2.39605007e-01, -2.03555112e-01, -2.45199591e-01]), sr=16000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditus.transform import Resampling\n",
    "\n",
    "resampled = Resampling(target_sr=16000)(audio)\n",
    "resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main transform in `auditus` is the `AudioEmbedding` transform. It takes an `AudioArray` and returns a tensor. Check out the [HuggingFace docs](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor) for more information on the available parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1214, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.5876,  0.2830, -0.7292,  0.7644, -1.1770])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditus.transform import AudioEmbedding\n",
    "\n",
    "emb = AudioEmbedding(return_tensors=\"pt\")(resampled)\n",
    "print(emb.shape)\n",
    "emb[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the embeddings, you often want to pool the embeddings to a single vector. `Pooling` supports `mean` and `max` pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.8619, 2.7183, 4.1288, 2.6302, 2.2177])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditus.transform import Pooling\n",
    "\n",
    "pooled = Pooling(pooling=\"max\")(emb)\n",
    "print(pooled.shape)\n",
    "pooled[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
