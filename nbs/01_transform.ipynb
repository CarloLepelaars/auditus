{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from fastcore.all import *\n",
    "from fasttransform import DisplayedTransform, Pipeline\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "from auditus.core import AudioArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioLoader Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AudioLoader` transform reads in audio file paths with a given sampling rate. The file is loaded into an `AudioArray` object, which contains a 1D NumPy array of the audio signal and the sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AudioLoader(DisplayedTransform):\n",
    "    def __init__(self, sr: int = None): store_attr()\n",
    "    def encodes(self, x:str) -> AudioArray: return self.load_audio(x, self.sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio(path, sr=None): \n",
    "        with sf.SoundFile(path) as f: return AudioArray(f.read(), sr if sr else f.samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test files are `.ogg` files with a sampling rate of 32kHz (`32_000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 32_000\n",
    "al = AudioLoader(sr=sr)\n",
    "test_eq(al.sr, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"../test_files\"\n",
    "file_paths = globtastic(test_dir, file_glob=\"*.ogg\")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = file_paths[-1]\n",
    "test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str -> AudioArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test file is a bird song from [Xeno Canto](https://xeno-canto.org/) of approximately 20 seconds. The length should be nearly $32000 \\times 20 = 640000$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_arr = al(test_path)\n",
    "test_eq(audio_arr.sr, sr)\n",
    "test_eq(audio_arr.shape, (632790,))\n",
    "audio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AST (Audio Transformer) model we use requires 16kHz audio. We can use `Resampling` to get audio with the correct sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Resampling(DisplayedTransform):\n",
    "    def __init__(self, target_sr: int):\n",
    "        store_attr()\n",
    "    def encodes(self, audio: AudioArray) -> AudioArray: return self.process_audio_array(audio)\n",
    "    \n",
    "    def process_audio_array(self, audio: AudioArray) -> AudioArray:\n",
    "        if audio.sr == self.target_sr: return audio\n",
    "        indices = np.linspace(0, len(audio.a) - 1, self._new_length(audio, self.target_sr))\n",
    "        resampled = np.interp(indices, np.arange(len(audio.a)), audio.a)\n",
    "        return AudioArray(resampled, self.target_sr)\n",
    "\n",
    "    def _new_length(self, audio: AudioArray, target_sr: int) -> int:\n",
    "        return int(len(audio.a) * (target_sr / audio.sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sr = 16_000\n",
    "r = Resampling(target_sr=target_sr)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new length is:\n",
    "\n",
    "$$l_{new} = l_{old} \\frac{sr_{new}}{sr_{old}}$$\n",
    "\n",
    ", where $l$ is the NumPy array length and $sr$ is the sampling rate. \n",
    "\n",
    "In our example:\n",
    "\n",
    "$$632790 \\frac{16000}{32000} = 632790 * 0.5 = 316395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_length = 316395\n",
    "test_eq(r._new_length(audio_arr, target_sr), expected_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = r(audio_arr)\n",
    "test_eq(resampled.sr, target_sr)\n",
    "test_eq(resampled.shape, (expected_length,))\n",
    "resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(resampled, rate=target_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AudioEmbedding` allows us to use HuggingFace Audio models as feature extractors. A great baseline model is the [Audio SpectrogramTransformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593) model, which is the default in `auditus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AudioEmbedding(DisplayedTransform):\n",
    "    def __init__(self, model_name: str = \"MIT/ast-finetuned-audioset-10-10-0.4593\", return_tensors: str = \"np\", **kwargs): \n",
    "        store_attr()\n",
    "        self.model = AutoFeatureExtractor.from_pretrained(model_name, **kwargs)\n",
    "\n",
    "    def encodes(self, x:AudioArray): return self.call_model(x.a, x.sr)\n",
    "    \n",
    "    def call_model(self, x, sr: int):\n",
    "        return self.model(x, sampling_rate=sr, return_tensors=self.return_tensors)['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AudioEmbedding(num_mel_bins=256)\n",
    "test_eq(ae.model.num_mel_bins, 256)\n",
    "test_eq(ae.model.sampling_rate, 16_000)\n",
    "ae.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = ae(resampled)\n",
    "test_eq(emb.shape, (1, 1024, 256))\n",
    "emb[0][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ae = AudioEmbedding(num_mel_bins=256, return_tensors=\"pt\")\n",
    "torch_emb = torch_ae(resampled)\n",
    "test_eq(torch_emb.shape, torch.Size([1, 1024, 256]))\n",
    "torch_emb[0][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any audio model on the HuggingFaceHub can be used to get audio embeddings. Here we use a [fine-tuned AST model](https://huggingface.co/xpariz10/ast-finetuned-audioset-10-10-0.4593_ft_env_aug_0-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ae = AudioEmbedding(model_name=\"xpariz10/ast-finetuned-audioset-10-10-0.4593_ft_env_aug_0-2\", num_mel_bins=256, return_tensors=\"np\")\n",
    "custom_emb = custom_ae(resampled)\n",
    "test_eq(custom_emb.shape, (1, 1024, 256))\n",
    "custom_emb[0][0][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_emb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pooling(DisplayedTransform):\n",
    "    def __init__(self, pooling: str = None):\n",
    "        assert pooling in [None, \"mean\", \"max\"], \"Pooling must be either None (no pooling), 'mean' or 'max'.\"\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, x:np.ndarray) -> np.ndarray: \n",
    "        if self.pooling is None: return x\n",
    "        elif self.pooling == \"mean\": return x.mean(axis=1)\n",
    "        elif self.pooling == \"max\": return x.max(axis=1)\n",
    "\n",
    "    def encodes(self, x:torch.Tensor) -> torch.Tensor: \n",
    "        if self.pooling is None: return x\n",
    "        # Torch aggregation also returns a tuple with max indices, so we need to unpack it\n",
    "        elif self.pooling == \"mean\": return x.mean(dim=1)[0]\n",
    "        elif self.pooling == \"max\": return x.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = Pooling(pooling=\"mean\")\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = np.array([[\n",
    "    [0.1, 0.2, 0.1],\n",
    "    [0.1, 0.2, 0.9],\n",
    "    [0.8, 0.6, 0.0]\n",
    "]])\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `pooling=None`, the input is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_pooler = Pooling()\n",
    "none_pooled = none_pooler(test_emb)\n",
    "test_eq(none_pooled, test_emb)\n",
    "none_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `pooling=\"mean\"`, the mean of each embedding is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooler = Pooling(pooling=\"mean\")\n",
    "mean_pooled = mean_pooler(test_emb)\n",
    "test_eq(mean_pooled, np.array([[1/3, 1/3, 1/3]]))\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `pooling=\"max\"`, the maximum of each embedding is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pooler = Pooling(pooling=\"max\")\n",
    "max_pooled = max_pooler(test_emb)\n",
    "test_eq(max_pooled, np.array([[0.8, 0.6, 0.9]]))\n",
    "max_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pooler can handle Torch tensors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_emb = torch.tensor(test_emb)\n",
    "torch_pooled = Pooling(pooling=\"mean\").encodes(torch_emb)\n",
    "test_eq(torch_pooled, torch.tensor([[1/3, 1/3, 1/3]], dtype=torch.float64))\n",
    "torch_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pooler can handle multiple embeddings at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_emb = np.array([test_emb[0]] * 2)\n",
    "test_eq(multi_emb.shape, (2, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pooled = Pooling(pooling=\"mean\").encodes(multi_emb)\n",
    "test_eq(multi_pooled, np.array([[1/3, 1/3, 1/3], \n",
    "                                [1/3, 1/3, 1/3]]))\n",
    "test_eq(multi_pooled.shape, (2, 3))\n",
    "multi_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compose a pipeline that loads an audio file with a sampling rate of 32kHz, resamples it to 16kHz, embeds it and max-pools the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([al, r, ae, max_pooler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = pipe(test_path)\n",
    "test_eq(emb.shape, (1, 256))\n",
    "emb[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AudioPipeline(Pipeline):\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"MIT/ast-finetuned-audioset-10-10-0.4593\", \n",
    "                 return_tensors: str = \"np\",\n",
    "                 target_sr: int = 16_000, \n",
    "                 pooling: str = \"max\", \n",
    "                 **kwargs):\n",
    "        super().__init__([\n",
    "            AudioLoader(),\n",
    "            Resampling(target_sr),\n",
    "            AudioEmbedding(model_name, return_tensors, **kwargs),\n",
    "            Pooling(pooling)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AudioPipeline(num_mel_bins=256, return_tensors=\"pt\")\n",
    "emb = pipe(test_path)\n",
    "test_eq(emb.shape, torch.Size([1, 256]))\n",
    "emb[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple audio files in Torch\n",
    "multi_emb = torch.stack([pipe(f).squeeze(0) for f in file_paths])\n",
    "test_eq(multi_emb.shape, torch.Size([2, 256]))\n",
    "multi_emb[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple audio files in NumPy\n",
    "pipe = AudioPipeline(num_mel_bins=256, return_tensors=\"np\")\n",
    "multi_emb = np.stack([pipe(f).squeeze(0) for f in file_paths])\n",
    "test_eq(multi_emb.shape, (2, 256))\n",
    "multi_emb[:, :5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
